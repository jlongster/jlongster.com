{"count":621,"tx0":536870912,"max-eid":89,"max-tx":536870913,"schema":"{\":block/string\" {}, \":block/md\" {}, \":post/featured-img\" {}, \":block/post\" {:db/valueType :db.type/ref, :db/cardinality :db.cardinality/one}, \":block/type\" {}, \":post/url\" {}, \":post/tags\" {:db/cardinality :db.cardinality/many}, \":block/order\" {}, \":block/meta\" {}, \":post/public\" {}, \":post/name\" {}, \":block/uuid\" {}, \":post/date\" {}}","attrs":[":block/md",":block/meta",":block/order",":block/post",":block/string",":block/type",":block/uuid",":post/date",":post/featured-img",":post/public",":post/subtitle",":post/title",":post/url"],"keywords":[],"eavt":[[1,7,[1,"#inst \"2024-07-08T00:00:00.000-00:00\""],1],[1,8,"",1],[1,9,false,1],[1,10,"",1],[1,11,"Welcome here -- the one thing that could",1],[1,12,"https://jlongster.com/little-learnings",1],[2,0,"fuck\n",1],[2,1,[1,"{}"],1],[2,3,1,1],[2,4,"fuck",1],[2,5,"paragraph",1],[2,6,"636ba5c4-ee4e-4d48-8ae6-f316788cb2f6",1],[3,0,"```js run\nimport * as Plot from \"https://cdn.jsdelivr.net/npm/@observablehq/plot@0.6/+esm\";\n\nwindow.Plot = Plot;\nwindow.range = (start, end) => new Array(end - start).fill(0).map((x, idx) => idx + start)\nwindow.deep_compare = (v1, v2) => {\n  return v1 === v2 || JSON.stringify(v1) === JSON.stringify(v2)\n}\nwindow.zip = (arr1, arr2) => {\n  return arr1.map((v, idx) => {\n    return [v, arr2[idx]]\n  })\n}\n\nconst defaultColors = [\n  \"#e60049\",\n  \"#0bb4ff\",\n  \"#50e991\"\n]\n\nconsole.log('installing graph')\nwindow.graph = (...funcs) => {\n  let options = {}\n  if(typeof funcs[funcs.length - 1] !== 'function') {\n    options = funcs[funcs.length - 1]\n    funcs = funcs.slice(0, -1)\n  }\n  \n  const domainX = options.domainX || [0, 100]\n  const domainY = options.domainY || [0, 100]\n  const xs = range(domainX[0], domainX[1])\n  const colors = options.colors || defaultColors;\n  \n  return Plot.plot({\n    marks: funcs.map((func, idx) => (\n      Plot.lineY(xs, {x: n => n, y: n => func(n), stroke: colors[idx % colors.length]})),\n    ).concat(\n      options.marks || []\n    ),\n    y: {domain: domainY},\n    style: {\n      fontSize: 15\n    }\n  })\n}\n\nwindow.log = func => {\n  const el = document.createElement('pre')\n  const output = (name, value, expected) => {\n    if(typeof name !== 'string') {\n      expected = value;\n      value = name;\n      name = 'value';\n    }\n\n    const d = document.createElement('code')\n\td.textContent = `${name}: ${JSON.stringify(value)}`\n    if(expected !== undefined) {\n\t  d.textContent += `, expected: ${JSON.stringify(expected)}`\n    }\n    d.textContent += '\\n'\n    el.appendChild(d);\n  }\n  func(output)\n  return el;\n}\n```\n",1],[3,1,[1,"{\"lang\" \"js\", \"run\" true}"],1],[3,2,1,1],[3,3,1,1],[3,4,"import * as Plot from \"https://cdn.jsdelivr.net/npm/@observablehq/plot@0.6/+esm\";\n\nwindow.Plot = Plot;\nwindow.range = (start, end) => new Array(end - start).fill(0).map((x, idx) => idx + start)\nwindow.deep_compare = (v1, v2) => {\n  return v1 === v2 || JSON.stringify(v1) === JSON.stringify(v2)\n}\nwindow.zip = (arr1, arr2) => {\n  return arr1.map((v, idx) => {\n    return [v, arr2[idx]]\n  })\n}\n\nconst defaultColors = [\n  \"#e60049\",\n  \"#0bb4ff\",\n  \"#50e991\"\n]\n\nconsole.log('installing graph')\nwindow.graph = (...funcs) => {\n  let options = {}\n  if(typeof funcs[funcs.length - 1] !== 'function') {\n    options = funcs[funcs.length - 1]\n    funcs = funcs.slice(0, -1)\n  }\n  \n  const domainX = options.domainX || [0, 100]\n  const domainY = options.domainY || [0, 100]\n  const xs = range(domainX[0], domainX[1])\n  const colors = options.colors || defaultColors;\n  \n  return Plot.plot({\n    marks: funcs.map((func, idx) => (\n      Plot.lineY(xs, {x: n => n, y: n => func(n), stroke: colors[idx % colors.length]})),\n    ).concat(\n      options.marks || []\n    ),\n    y: {domain: domainY},\n    style: {\n      fontSize: 15\n    }\n  })\n}\n\nwindow.log = func => {\n  const el = document.createElement('pre')\n  const output = (name, value, expected) => {\n    if(typeof name !== 'string') {\n      expected = value;\n      value = name;\n      name = 'value';\n    }\n\n    const d = document.createElement('code')\n\td.textContent = `${name}: ${JSON.stringify(value)}`\n    if(expected !== undefined) {\n\t  d.textContent += `, expected: ${JSON.stringify(expected)}`\n    }\n    d.textContent += '\\n'\n    el.appendChild(d);\n  }\n  func(output)\n  return el;\n}",1],[3,5,"code",1],[3,6,"e1f8f90f-f99f-489e-879a-63c311ffcce0",1],[4,0,"### Chapter 1\n",1],[4,1,[1,"{}"],1],[4,2,2,1],[4,3,1,1],[4,4,"Chapter 1",1],[4,5,"heading",1],[4,6,"0e250989-d37d-494e-9727-272797fab6c5",1],[5,0,"#### 1.5: graph a line with *w* and *b* parameters\n",1],[5,1,[1,"{}"],1],[5,2,3,1],[5,3,1,1],[5,4,"1.5: graph a line with w and b parameters",1],[5,5,"heading",1],[5,6,"d884d737-2b1a-4435-82e3-25f83a6e1b29",1],[6,0,"render:: js-element\nsource:: true\n",1],[6,1,[1,"{}"],1],[6,2,4,1],[6,3,1,1],[6,4,"render:: js-element\nsource:: true",1],[6,5,"paragraph",1],[6,6,"920d5693-d97b-4d19-a4cd-b12b43a90cce",1],[7,0,"This thing\n",1],[7,1,[1,"{}"],1],[7,2,5,1],[7,3,1,1],[7,4,"This thing",1],[7,5,"paragraph",1],[7,6,"fc3a7bdc-f871-4ece-883b-d34fcb054dd9",1],[8,0,"```js run show\nconst f = (w, b) => x => w * x + b\n\nrender(graph(f(1, 10)))\nrender(graph(f(10, 0)))\n```\n",1],[8,1,[1,"{\"lang\" \"js\", \"run\" true, \"show\" true}"],1],[8,2,6,1],[8,3,1,1],[8,4,"const f = (w, b) => x => w * x + b\n\nrender(graph(f(1, 10)))\nrender(graph(f(10, 0)))",1],[8,5,"code",1],[8,6,"5efeaf17-4203-44db-a236-8c8d4d8c36ea",1],[9,0,"#### 1.14: reverse the order  to make *x* an argument and *w* and *p* parameters\n",1],[9,1,[1,"{}"],1],[9,2,7,1],[9,3,1,1],[9,4,"1.14: reverse the order  to make x an argument and w and p parameters",1],[9,5,"heading",1],[9,6,"e490faf2-7bbe-41ab-b59f-eab90847879a",1],[10,0,"render:: js-element\nsource:: true\n",1],[10,1,[1,"{}"],1],[10,2,8,1],[10,3,1,1],[10,4,"render:: js-element\nsource:: true",1],[10,5,"paragraph",1],[10,6,"bc23b156-cc08-411e-9004-3390c594c63d",1],[11,0,"```js\n// A version where w and b become parameters _after_ `x`\nconst f = x => (w, b) => w * x + b\n\nreturn graph(x => f(x)(2, 20))\n```\n",1],[11,1,[1,"{\"lang\" \"js\"}"],1],[11,2,9,1],[11,3,1,1],[11,4,"// A version where w and b become parameters _after_ `x`\nconst f = x => (w, b) => w * x + b\n\nreturn graph(x => f(x)(2, 20))",1],[11,5,"code",1],[11,6,"4801c6ae-6e72-4532-9fa3-dba97df01398",1],[12,0,"`x` is the argument of the line, while `w` and `b` which come after are `parameters`\n",1],[12,1,[1,"{}"],1],[12,2,10,1],[12,3,1,1],[12,4,"x is the argument of the line, while w and b which come after are parameters",1],[12,5,"paragraph",1],[12,6,"ca3db4d9-442f-4c74-8a16-e9d8da620168",1],[13,0,"#### 1.19 plot the xs, ys dataset\n",1],[13,1,[1,"{}"],1],[13,2,11,1],[13,3,1,1],[13,4,"1.19 plot the xs, ys dataset",1],[13,5,"heading",1],[13,6,"7eba2a93-8a48-4339-8d69-6539880c7e68",1],[14,0,"render:: js-element\nsource:: true\n",1],[14,1,[1,"{}"],1],[14,2,12,1],[14,3,1,1],[14,4,"render:: js-element\nsource:: true",1],[14,5,"paragraph",1],[14,6,"47f3b564-0c94-456d-9374-a415aa06e0bb",1],[15,0,"```js\nconst xs = [2, 1, 4, 3]\nconst ys = [1.8, 1.2, 4.2, 3.3]\n\nreturn graph({\n  marks: Plot.dot(zip(xs, ys), {x: n => n[0], y: n => n[1], fill: 'black'}),\n  domainX: [0, 5],\n  domainY: [0, 5]\n})\n```\n",1],[15,1,[1,"{\"lang\" \"js\"}"],1],[15,2,13,1],[15,3,1,1],[15,4,"const xs = [2, 1, 4, 3]\nconst ys = [1.8, 1.2, 4.2, 3.3]\n\nreturn graph({\n  marks: Plot.dot(zip(xs, ys), {x: n => n[0], y: n => n[1], fill: 'black'}),\n  domainX: [0, 5],\n  domainY: [0, 5]\n})",1],[15,5,"code",1],[15,6,"4c27f861-a974-4572-b8a4-bed90c9fbe7d",1],[16,0,"**Rule of parameters: every parameter is a number**\nGiven x and y, or arguments to a function, we can walk backwards and figure out the parameters and then use that to predict other `y` values for a given `x`\nθ is the *parameter set*\nGiven θ, there parameters if it referred to as θ\\_{ 1}, θ\\_{ 2}, etc\n",1],[16,1,[1,"{}"],1],[16,2,14,1],[16,3,1,1],[16,4,"Rule of parameters: every parameter is a number\nGiven x and y, or arguments to a function, we can walk backwards and figure out the parameters and then use that to predict other y values for a given x\nθ is the parameter set\nGiven θ, there parameters if it referred to as θ_{ 1}, θ_{ 2}, etc",1],[16,5,"paragraph",1],[16,6,"8a994fa7-9bcc-4463-98d7-3864e69b094e",1],[17,0,"```js\nconst f = θ => (θ_1, θ_2) => θ_1 * x + θ_2\n```\n",1],[17,1,[1,"{\"lang\" \"js\"}"],1],[17,2,15,1],[17,3,1,1],[17,4,"const f = θ => (θ_1, θ_2) => θ_1 * x + θ_2",1],[17,5,"code",1],[17,6,"b491bead-c2da-4cee-b0a0-0a19b0cda954",1],[18,0,"### Chapter 2\n",1],[18,1,[1,"{}"],1],[18,2,16,1],[18,3,1,1],[18,4,"Chapter 2",1],[18,5,"heading",1],[18,6,"7c6efdf9-43fd-4791-bd41-3a7cdd1bbf69",1],[19,0,"\"Scalars\" are real numbers\nA \"tensor\" is a vector of scalars: `[2.0 1.0 4.3 4.2]`\nIn the book this uses tensor^{ 1} with a superscript\nTensors can be nested, and the superscript indicates the level of \"nested\"\n\"elements\" are the individual values in the tensor\nI think tensor^{ 1} (with the 1 superscript) specifically means a vector of scalars, and higher tensors have tensors as elements\n**All tensors^{ m} must have the same number of elements**\nA scalar is a tensor^{ 0}\n`9` is tensor^{ 0}\n`[9 9 9]` is tensor^{ 1}\n`[[9 9 9] [9 9 9]]` is tensor^{ 2}\n",1],[19,1,[1,"{}"],1],[19,2,17,1],[19,3,1,1],[19,4,"\"Scalars\" are real numbers\nA \"tensor\" is a vector of scalars: [2.0 1.0 4.3 4.2]\nIn the book this uses tensor^{ 1} with a superscript\nTensors can be nested, and the superscript indicates the level of \"nested\"\n\"elements\" are the individual values in the tensor\nI think tensor^{ 1} (with the 1 superscript) specifically means a vector of scalars, and higher tensors have tensors as elements\nAll tensors^{ m} must have the same number of elements\nA scalar is a tensor^{ 0}\n9 is tensor^{ 0}\n[9 9 9] is tensor^{ 1}\n[[9 9 9] [9 9 9]] is tensor^{ 2}",1],[19,5,"paragraph",1],[19,6,"46e2552a-b2e5-4ac4-90d1-aca3532abfd6",1],[20,0,"#### 2.25: define a function that finds the rank of a tensor\n",1],[20,1,[1,"{}"],1],[20,2,18,1],[20,3,1,1],[20,4,"2.25: define a function that finds the rank of a tensor",1],[20,5,"heading",1],[20,6,"bb0a44c1-3144-4c8b-ac12-73958a07ab21",1],[21,0,"render:: js-element\nsource:: true\n",1],[21,1,[1,"{}"],1],[21,2,19,1],[21,3,1,1],[21,4,"render:: js-element\nsource:: true",1],[21,5,"paragraph",1],[21,6,"818c4aab-44a3-472f-9fa7-863e69c1eb4f",1],[22,0,"```js\nwindow.scalarp = v => typeof v === 'number'\nwindow.rank = t => scalarp(t) ? 0 : 1 + rank(t[0]);\n\nreturn log(output => {\n  output(rank(4), 0)\n  output(rank([4, 1]), 1)\n  output(rank([[4, 1], [3, 6]]), 2)\n  output(rank([[[[[3]]]]]), 5)\n})\n```\n",1],[22,1,[1,"{\"lang\" \"js\"}"],1],[22,2,20,1],[22,3,1,1],[22,4,"window.scalarp = v => typeof v === 'number'\nwindow.rank = t => scalarp(t) ? 0 : 1 + rank(t[0]);\n\nreturn log(output => {\n  output(rank(4), 0)\n  output(rank([4, 1]), 1)\n  output(rank([[4, 1], [3, 6]]), 2)\n  output(rank([[[[[3]]]]]), 5)\n})",1],[22,5,"code",1],[22,6,"30dbb874-7db1-43ac-b62c-b2f0cbb5d0c3",1],[23,0,"#### 2.37 define a function that finds the shape of a tensor *t*\n",1],[23,1,[1,"{}"],1],[23,2,21,1],[23,3,1,1],[23,4,"2.37 define a function that finds the shape of a tensor t",1],[23,5,"heading",1],[23,6,"ef62449a-6741-41d5-84c3-a7ecc2071674",1],[24,0,"render:: js-element\nsource:: true\n",1],[24,1,[1,"{}"],1],[24,2,22,1],[24,3,1,1],[24,4,"render:: js-element\nsource:: true",1],[24,5,"paragraph",1],[24,6,"83ab4953-0b86-4ab1-9f0d-89865d7dda81",1],[25,0,"```js\nwindow.shape = (t, acc=[]) => {\n  if(!scalarp(t)) {\n    acc.push(t.length);\n    shape(t[0], acc)\n  }\n  return acc;\n}\n\nreturn log(output => {\n  output(shape(5), [])\n  output(shape([[4, 1], [3, 6], [2, 9]]), [3, 2])\n  output(shape([[[[[3]]]]]), [1, 1, 1, 1, 1])\n})\n```\n",1],[25,1,[1,"{\"lang\" \"js\"}"],1],[25,2,23,1],[25,3,1,1],[25,4,"window.shape = (t, acc=[]) => {\n  if(!scalarp(t)) {\n    acc.push(t.length);\n    shape(t[0], acc)\n  }\n  return acc;\n}\n\nreturn log(output => {\n  output(shape(5), [])\n  output(shape([[4, 1], [3, 6], [2, 9]]), [3, 2])\n  output(shape([[[[[3]]]]]), [1, 1, 1, 1, 1])\n})",1],[25,5,"code",1],[25,6,"d4b862ac-23da-4006-82b9-919fc1bdbc44",1],[26,0,"**Lists have *members* while non-scalar tensors have *elements***\n",1],[26,1,[1,"{}"],1],[26,2,24,1],[26,3,1,1],[26,4,"Lists have members while non-scalar tensors have elements",1],[26,5,"paragraph",1],[26,6,"b763ca9f-6425-47bd-8272-74cfcd0935b5",1],[27,0,"#### 2.42: how are `rank` and `shape` related?\n",1],[27,1,[1,"{}"],1],[27,2,25,1],[27,3,1,1],[27,4,"2.42: how are rank and shape related?",1],[27,5,"heading",1],[27,6,"7a93411a-bef8-459b-a3b3-a30d21f77267",1],[28,0,"render:: js-element\nsource:: true\n",1],[28,1,[1,"{}"],1],[28,2,26,1],[28,3,1,1],[28,4,"render:: js-element\nsource:: true",1],[28,5,"paragraph",1],[28,6,"4ed709e7-5175-4c11-a2f7-802b6cc61c60",1],[29,0,"```js\n// `rank` could also be defined as the length of `shape`\nwindow.rank2 = t => shape(t).length;\n\nreturn log(output => {\n  output(rank2(4), 0)\n  output(rank2([4, 1]), 1)\n  output(rank2([[4, 1], [3, 6]]), 2)\n  output(rank2([[[[[3]]]]]), 5)\n})\n```\n",1],[29,1,[1,"{\"lang\" \"js\"}"],1],[29,2,27,1],[29,3,1,1],[29,4,"// `rank` could also be defined as the length of `shape`\nwindow.rank2 = t => shape(t).length;\n\nreturn log(output => {\n  output(rank2(4), 0)\n  output(rank2([4, 1]), 1)\n  output(rank2([[4, 1], [3, 6]]), 2)\n  output(rank2([[[[[3]]]]]), 5)\n})",1],[29,5,"code",1],[29,6,"ea8a7d7e-a2b6-4cc4-9ba5-fdd9efc4bf31",1],[30,0,"### Interlude I\n",1],[30,1,[1,"{}"],1],[30,2,28,1],[30,3,1,1],[30,4,"Interlude I",1],[30,5,"heading",1],[30,6,"15c857c2-d6f0-4575-b04e-e393023651a6",1],[31,0,"#### I.8: addition of tensors\n",1],[31,1,[1,"{}"],1],[31,2,29,1],[31,3,1,1],[31,4,"I.8: addition of tensors",1],[31,5,"heading",1],[31,6,"3ba480e8-2d75-4947-9b02-76f97aeb1a2c",1],[32,0,"render:: js-element\n",1],[32,1,[1,"{}"],1],[32,2,30,1],[32,3,1,1],[32,4,"render:: js-element",1],[32,5,"paragraph",1],[32,6,"dcc7b431-efa2-422f-a2ae-3a495f4f94ff",1],[33,0,"```js\nconst tadd = (t1, t2) => t1.map((v, idx) =>\n  scalarp(v) ? v + t2[idx] : tadd(v, t2[idx])\n)\n\nreturn log(output => {\n  output(tadd([1, 2, 3], [4, 5, 6]), [5, 7, 9])\n  output(tadd([[1, 2], [1, 2]], [[3, 4], [3, 4]]), [[4, 6], [4, 6]])\n  output(tadd([[[[[5]]]]], [[[[[1]]]]]), [[[[[6]]]]])\n})\n```\n",1],[33,1,[1,"{\"lang\" \"js\"}"],1],[33,2,31,1],[33,3,1,1],[33,4,"const tadd = (t1, t2) => t1.map((v, idx) =>\n  scalarp(v) ? v + t2[idx] : tadd(v, t2[idx])\n)\n\nreturn log(output => {\n  output(tadd([1, 2, 3], [4, 5, 6]), [5, 7, 9])\n  output(tadd([[1, 2], [1, 2]], [[3, 4], [3, 4]]), [[4, 6], [4, 6]])\n  output(tadd([[[[[5]]]]], [[[[[1]]]]]), [[[[[6]]]]])\n})",1],[33,5,"code",1],[33,6,"6854de50-7605-4ef6-8a03-0b0c5c7bb032",1],[34,0,"Currently, tensors must be of the same shape to add them with `+`\nMaking `+` work on tensors of arbitrary rank is called an *extension* of `+`\nFunctions built using extensions are called *extended functions*\nNote: extension applies both to making an operator work on a single argument that has an arbitrary rank, and also making it work with two arguments that may have different ranks. A non-extended operator is one that only works with one specific rank\n",1],[34,1,[1,"{}"],1],[34,2,32,1],[34,3,1,1],[34,4,"Currently, tensors must be of the same shape to add them with +\nMaking + work on tensors of arbitrary rank is called an extension of +\nFunctions built using extensions are called extended functions\nNote: extension applies both to making an operator work on a single argument that has an arbitrary rank, and also making it work with two arguments that may have different ranks. A non-extended operator is one that only works with one specific rank",1],[34,5,"paragraph",1],[34,6,"c612c5ff-08aa-4930-971c-c83fbcc4e36a",1],[35,0,"#### I.14 extended addition\n",1],[35,1,[1,"{}"],1],[35,2,33,1],[35,3,1,1],[35,4,"I.14 extended addition",1],[35,5,"heading",1],[35,6,"753e456d-eac0-47e4-b72c-ea395e5e7997",1],[36,0,"render:: js-element\n",1],[36,1,[1,"{}"],1],[36,2,34,1],[36,3,1,1],[36,4,"render:: js-element",1],[36,5,"paragraph",1],[36,6,"599e9f8c-2218-4ab3-9c79-2997c9f4e875",1],[37,0,"```js\nconst tadd = (t1, t2) => t1.map((v, idx) =>\n  scalarp(v) ? v + t2[idx] : tadd(v, t2[idx])\n)\nconst tadd_ = (t1, t2) => {\n  if(deep_compare(shape(t1), shape(t2))) {\n    return tadd(t1, t2);\n  }\n  \n  return t2.map(v => tadd_(t1, v));\n}\n\n\nreturn log(output => {\n  output(tadd_([1, 2], [[0, 0], [1, 1]]), [[1, 2], [2, 3]])\n  output(tadd_([1, 2], [[[[0, 0], [1, 1]]]]), [[[[1, 2], [2, 3]]]])\n})\n```\n",1],[37,1,[1,"{\"lang\" \"js\"}"],1],[37,2,35,1],[37,3,1,1],[37,4,"const tadd = (t1, t2) => t1.map((v, idx) =>\n  scalarp(v) ? v + t2[idx] : tadd(v, t2[idx])\n)\nconst tadd_ = (t1, t2) => {\n  if(deep_compare(shape(t1), shape(t2))) {\n    return tadd(t1, t2);\n  }\n  \n  return t2.map(v => tadd_(t1, v));\n}\n\n\nreturn log(output => {\n  output(tadd_([1, 2], [[0, 0], [1, 1]]), [[1, 2], [2, 3]])\n  output(tadd_([1, 2], [[[[0, 0], [1, 1]]]]), [[[[1, 2], [2, 3]]]])\n})",1],[37,5,"code",1],[37,6,"9982249d-20bc-4e3c-8e8c-f806c72eaae3",1],[38,0,"We can do this with all mathematical operations: `*`, `sqrt`, etc\nIt looks like the order shouldn't matter. With our last definition, we are assuming `t2` has a higher rank, but we can make it work so that the higher rank can be in either position\nrender:: js-element\n",1],[38,1,[1,"{}"],1],[38,2,36,1],[38,3,1,1],[38,4,"We can do this with all mathematical operations: *, sqrt, etc\nIt looks like the order shouldn't matter. With our last definition, we are assuming t2 has a higher rank, but we can make it work so that the higher rank can be in either position\nrender:: js-element",1],[38,5,"paragraph",1],[38,6,"d27641a2-ef8b-4386-ba1d-e98063d5fca3",1],[39,0,"```js\nconst tadd = (t1, t2) => {\n  if(scalarp(t1)) {\n    return t1 + t2;\n  }\n  return t1.map((v, idx) => tadd(v, t2[idx]))\n}\nconst tadd_ = (t1, t2) => {\n  if(deep_compare(shape(t1), shape(t2))) {\n    return tadd(t1, t2);\n  }\n  \n  const [lower, higher] = rank(t1) < rank(t2) ? [t1, t2] : [t2, t1];\n  return higher.map(v => tadd_(lower, v));\n}\n\nwindow.tadd_ = tadd_;\n\nreturn log(output => {\n  output(tadd_([1, 2], [[0, 0], [1, 1]]), [[1, 2], [2, 3]])\n  output(tadd_([[[[0, 0], [1, 1]]]], [1, 2]), [[[[1, 2], [2, 3]]]])\n  output(tadd_([[[[0, 0], [1, 1]]]], 5), [[[[5, 5], [6, 6]]]])\n})\n```\n",1],[39,1,[1,"{\"lang\" \"js\"}"],1],[39,2,37,1],[39,3,1,1],[39,4,"const tadd = (t1, t2) => {\n  if(scalarp(t1)) {\n    return t1 + t2;\n  }\n  return t1.map((v, idx) => tadd(v, t2[idx]))\n}\nconst tadd_ = (t1, t2) => {\n  if(deep_compare(shape(t1), shape(t2))) {\n    return tadd(t1, t2);\n  }\n  \n  const [lower, higher] = rank(t1) < rank(t2) ? [t1, t2] : [t2, t1];\n  return higher.map(v => tadd_(lower, v));\n}\n\nwindow.tadd_ = tadd_;\n\nreturn log(output => {\n  output(tadd_([1, 2], [[0, 0], [1, 1]]), [[1, 2], [2, 3]])\n  output(tadd_([[[[0, 0], [1, 1]]]], [1, 2]), [[[[1, 2], [2, 3]]]])\n  output(tadd_([[[[0, 0], [1, 1]]]], 5), [[[[5, 5], [6, 6]]]])\n})",1],[39,5,"code",1],[39,6,"7024f66f-26b9-4dda-a7c8-4b1c5c93440b",1],[40,0,"Not all math operations descend; for example sum^{ 1}\nsum^{ 1} has a superscript to make it clear it always expects a tensor\nrender:: js-element\n",1],[40,1,[1,"{}"],1],[40,2,38,1],[40,3,1,1],[40,4,"Not all math operations descend; for example sum^{ 1}\nsum^{ 1} has a superscript to make it clear it always expects a tensor\nrender:: js-element",1],[40,5,"paragraph",1],[40,6,"2468a415-4178-47aa-a507-063f2d6b0974",1],[41,0,"```js\nconst tsum = t => {\n  let total = 0;\n  for(let i=0; i<t.length; i++) {\n    total += t[i]\n  }\n  return total\n}\n\nreturn log(output => {\n  output(tsum([1, 2, 3]), 6)\n  output(tsum([]), 0)\n  output(tsum([5, 5, 5, 5, 5]), 25)\n})\n```\n",1],[41,1,[1,"{\"lang\" \"js\"}"],1],[41,2,39,1],[41,3,1,1],[41,4,"const tsum = t => {\n  let total = 0;\n  for(let i=0; i<t.length; i++) {\n    total += t[i]\n  }\n  return total\n}\n\nreturn log(output => {\n  output(tsum([1, 2, 3]), 6)\n  output(tsum([]), 0)\n  output(tsum([5, 5, 5, 5, 5]), 25)\n})",1],[41,5,"code",1],[41,6,"59e857c8-fc9c-453a-8591-d278376a88b0",1],[42,0,"sum is the extended version of sum^{ 1} which descends until it fins a tensor^{ 1}\nrender:: js-element\n",1],[42,1,[1,"{}"],1],[42,2,40,1],[42,3,1,1],[42,4,"sum is the extended version of sum^{ 1} which descends until it fins a tensor^{ 1}\nrender:: js-element",1],[42,5,"paragraph",1],[42,6,"d2d38d3c-0cba-41d2-91f9-5db56cbfbd3d",1],[43,0,"```js\nconst tsum_ = t => {\n  if(rank(t) > 1) {\n    return t.map(x => tsum_(x))\n  }\n  \n  let total = 0;\n  for(let i=0; i<t.length; i++) {\n    total += t[i]\n  }\n  return total\n}\n\nwindow.tsum_ = tsum_\n\nreturn log(output => {\n  output(tsum_([[1, 2, 3], [4, 5, 6]]), [6, 15])\n})\n```\n",1],[43,1,[1,"{\"lang\" \"js\"}"],1],[43,2,41,1],[43,3,1,1],[43,4,"const tsum_ = t => {\n  if(rank(t) > 1) {\n    return t.map(x => tsum_(x))\n  }\n  \n  let total = 0;\n  for(let i=0; i<t.length; i++) {\n    total += t[i]\n  }\n  return total\n}\n\nwindow.tsum_ = tsum_\n\nreturn log(output => {\n  output(tsum_([[1, 2, 3], [4, 5, 6]]), [6, 15])\n})",1],[43,5,"code",1],[43,6,"b9887ce0-cb63-40e2-988a-0aade295b0f3",1],[44,0,"**Rule of sum: a tensor *t* with a rank *r*, the rank of *sum(t)* is *r - 1***\nLet's add more operations\n**tsub:**\nrender:: js-element\n",1],[44,1,[1,"{}"],1],[44,2,42,1],[44,3,1,1],[44,4,"Rule of sum: a tensor t with a rank r, the rank of sum(t) is r - 1\nLet's add more operations\ntsub:\nrender:: js-element",1],[44,5,"paragraph",1],[44,6,"3d955a3a-1236-4663-8348-778a5b75fe8e",1],[45,0,"```js\nconst tsub_ = (t1, t2) => {\n  const r1 = rank(t1);\n  const r2 = rank(t2);\n  \n  if(r1 === r2) {\n    if(scalarp(t1)) {\n      return t1 - t2;\n    }\n    return t1.map((v, idx) => tsub_(v, t2[idx]))\n  }\n  else if(r1 < r2) {\n    return t2.map(t => tsub_(t1, t))\n  }\n  else {\n    return t1.map(t => tsub_(t, t2))\n  }\n}\n\nwindow.tsub_ = tsub_;\n\nreturn log(output => {\n  output(tsub_([1, 2], [[0, 0], [1, 1]]), [[1, 2], [0, 1]])\n  output(tsub_([[[[0, 0], [1, 1]]]], [1, 2]), [[[[-1, -2], [0, -1]]]])\n  output(tsub_([[[[0, 0], [1, 1]]]], 5), [[[[-5, -5], [-4, -4]]]])\n})\n```\n",1],[45,1,[1,"{\"lang\" \"js\"}"],1],[45,2,43,1],[45,3,1,1],[45,4,"const tsub_ = (t1, t2) => {\n  const r1 = rank(t1);\n  const r2 = rank(t2);\n  \n  if(r1 === r2) {\n    if(scalarp(t1)) {\n      return t1 - t2;\n    }\n    return t1.map((v, idx) => tsub_(v, t2[idx]))\n  }\n  else if(r1 < r2) {\n    return t2.map(t => tsub_(t1, t))\n  }\n  else {\n    return t1.map(t => tsub_(t, t2))\n  }\n}\n\nwindow.tsub_ = tsub_;\n\nreturn log(output => {\n  output(tsub_([1, 2], [[0, 0], [1, 1]]), [[1, 2], [0, 1]])\n  output(tsub_([[[[0, 0], [1, 1]]]], [1, 2]), [[[[-1, -2], [0, -1]]]])\n  output(tsub_([[[[0, 0], [1, 1]]]], 5), [[[[-5, -5], [-4, -4]]]])\n})",1],[45,5,"code",1],[45,6,"28dba9ae-ac73-4e83-b5b5-01c15d7d97c2",1],[46,0,"**tmul:**\nrender:: js-element\n",1],[46,1,[1,"{}"],1],[46,2,44,1],[46,3,1,1],[46,4,"tmul:\nrender:: js-element",1],[46,5,"paragraph",1],[46,6,"a8dd57f9-4952-4ec8-8be4-bcb322532e61",1],[47,0,"```js\nconst tmul = (t1, t2) => {\n  if(scalarp(t1)) {\n    return t1 * t2;\n  }\n  return t1.map((v, idx) => tmul(v, t2[idx]))\n}\n\nconst tmul_ = (t1, t2) => {\n  if(deep_compare(shape(t1), shape(t2))) {\n    return tmul(t1, t2);\n  }\n  \n  const [lower, higher] = rank(t1) < rank(t2) ? [t1, t2] : [t2, t1];\n  return higher.map(v => tmul_(lower, v));\n}\n\nwindow.tmul_ = tmul_\n\nreturn log(output => {\n  output(tmul_([2, 2, 3], [4, 5, 6]), [8, 10, 18])\n  output(tmul_([[2, 3], [4, 5]], [[2, 2], [2, 2]]), [[4, 6], [8, 10]])\n  output(tmul_([[2, 3], [4, 5]], 2), [[4, 6], [8, 10]])\n})\n```\n",1],[47,1,[1,"{\"lang\" \"js\"}"],1],[47,2,45,1],[47,3,1,1],[47,4,"const tmul = (t1, t2) => {\n  if(scalarp(t1)) {\n    return t1 * t2;\n  }\n  return t1.map((v, idx) => tmul(v, t2[idx]))\n}\n\nconst tmul_ = (t1, t2) => {\n  if(deep_compare(shape(t1), shape(t2))) {\n    return tmul(t1, t2);\n  }\n  \n  const [lower, higher] = rank(t1) < rank(t2) ? [t1, t2] : [t2, t1];\n  return higher.map(v => tmul_(lower, v));\n}\n\nwindow.tmul_ = tmul_\n\nreturn log(output => {\n  output(tmul_([2, 2, 3], [4, 5, 6]), [8, 10, 18])\n  output(tmul_([[2, 3], [4, 5]], [[2, 2], [2, 2]]), [[4, 6], [8, 10]])\n  output(tmul_([[2, 3], [4, 5]], 2), [[4, 6], [8, 10]])\n})",1],[47,5,"code",1],[47,6,"a6b95007-bffa-45e2-b4ac-3eedeb4cec3f",1],[48,0,"**tsqr:**\nrender:: js-element\n",1],[48,1,[1,"{}"],1],[48,2,46,1],[48,3,1,1],[48,4,"tsqr:\nrender:: js-element",1],[48,5,"paragraph",1],[48,6,"77f1f4b8-c1aa-445a-96c1-cc1074f7b4df",1],[49,0,"```js\nconst tsqr_ = (t1) => {\n  if(scalarp(t1)) {\n    return t1 * t1;\n  }\n  return t1.map(v => tsqr_(v))\n}\n\n\nwindow.tsqr_ = tsqr_\n\nreturn log(output => {\n  output(tsqr_([2, 2, 3]), [4, 4, 9])\n})\n```\n",1],[49,1,[1,"{\"lang\" \"js\"}"],1],[49,2,47,1],[49,3,1,1],[49,4,"const tsqr_ = (t1) => {\n  if(scalarp(t1)) {\n    return t1 * t1;\n  }\n  return t1.map(v => tsqr_(v))\n}\n\n\nwindow.tsqr_ = tsqr_\n\nreturn log(output => {\n  output(tsqr_([2, 2, 3]), [4, 4, 9])\n})",1],[49,5,"code",1],[49,6,"08bc2486-c6b4-40e5-b85f-c6aebe6a399e",1],[50,0,"### Chapter 3\n",1],[50,1,[1,"{}"],1],[50,2,48,1],[50,3,1,1],[50,4,"Chapter 3",1],[50,5,"heading",1],[50,6,"57a24be1-65f5-498c-b15b-94d13b8ac389",1],[51,0,"fitting: a well-fitted Θ is one the finds the best fit for a given data set\nLet's take this line equation again:\nrender:: js\nsource:: true\n",1],[51,1,[1,"{}"],1],[51,2,49,1],[51,3,1,1],[51,4,"fitting: a well-fitted Θ is one the finds the best fit for a given data set\nLet's take this line equation again:\nrender:: js\nsource:: true",1],[51,5,"paragraph",1],[51,6,"966f7da6-3995-49d7-a6ab-f82f5a7f7169",1],[52,0,"```js\nwindow.line = x => (w, b) => tadd_(tmul_(w, x), b)\n```\n",1],[52,1,[1,"{\"lang\" \"js\"}"],1],[52,2,50,1],[52,3,1,1],[52,4,"window.line = x => (w, b) => tadd_(tmul_(w, x), b)",1],[52,5,"code",1],[52,6,"4aa30f4c-d56a-40c7-b893-35fd60db51c5",1],[53,0,"We start with `w` (or Θ\\_{ 0}) and `b` (or Θ\\_{ 1}) both being `0.0`\nrender:: js-element\nsource:: true\n",1],[53,1,[1,"{}"],1],[53,2,51,1],[53,3,1,1],[53,4,"We start with w (or Θ_{ 0}) and b (or Θ_{ 1}) both being 0.0\nrender:: js-element\nsource:: true",1],[53,5,"paragraph",1],[53,6,"fb21cdff-65af-410c-8d1b-1b6f139f9557",1],[54,0,"```js\nreturn log(output => {\n  output(line([2, 1, 4, 3])(0, 0))\n})\n```\n",1],[54,1,[1,"{\"lang\" \"js\"}"],1],[54,2,52,1],[54,3,1,1],[54,4,"return log(output => {\n  output(line([2, 1, 4, 3])(0, 0))\n})",1],[54,5,"code",1],[54,6,"55a20423-7f35-4b34-a139-a96909752da0",1],[55,0,"The **loss** is how far away our parameters are. The best fit would be `loss` as close to 0 as possible\nHow do you calculate loss?\nFirst step: `line-ys - predicted-line-ys`\nwhere `predicted-line-ys` is `line(line-xs)(Θ_1,  Θ_2})`\n`line-ys - line(line-xs)(Θ_1,  Θ_2})`\nThat produces a tensor^{ 1} though -- how do we get a scalar?\nWe sum it together, but also need to square it to get rid of negative values\n`sum(sqr(ys - pred-ys))`\nrender:: js-element\nsource:: true\n",1],[55,1,[1,"{}"],1],[55,2,53,1],[55,3,1,1],[55,4,"The loss is how far away our parameters are. The best fit would be loss as close to 0 as possible\nHow do you calculate loss?\nFirst step: line-ys - predicted-line-ys\nwhere predicted-line-ys is line(line-xs)(Θ_1,  Θ_2})\nline-ys - line(line-xs)(Θ_1,  Θ_2})\nThat produces a tensor^{ 1} though -- how do we get a scalar?\nWe sum it together, but also need to square it to get rid of negative values\nsum(sqr(ys - pred-ys))\nrender:: js-element\nsource:: true",1],[55,5,"paragraph",1],[55,6,"d604c024-86a7-42e5-a934-e79b627c746d",1],[56,0,"```js\nwindow.l2loss = target => (\n  (xs, ys) => {\n    return (...params) => {\n      const pred_ys = target(xs)(params[0], params[1]);\n      return tsum_(tsqr_(tsub_(ys, pred_ys)));\n    }\n  }\n)\n\nreturn log(output => {\n  const loss1 = l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(0, 0)\n  const loss2 = l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(.0099, 0)\n  output(loss1)\n  output(loss2)\n  output(\"difference\", loss2 - loss1)\n})\n```\n",1],[56,1,[1,"{\"lang\" \"js\"}"],1],[56,2,54,1],[56,3,1,1],[56,4,"window.l2loss = target => (\n  (xs, ys) => {\n    return (...params) => {\n      const pred_ys = target(xs)(params[0], params[1]);\n      return tsum_(tsqr_(tsub_(ys, pred_ys)));\n    }\n  }\n)\n\nreturn log(output => {\n  const loss1 = l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(0, 0)\n  const loss2 = l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(.0099, 0)\n  output(loss1)\n  output(loss2)\n  output(\"difference\", loss2 - loss1)\n})",1],[56,5,"code",1],[56,6,"ebfa768e-fb1f-44d7-8f41-e8b40c3a68fd",1],[57,0,"Θ is the *parameter set*, it has nothing to do with the `line` function. It's just a list or vector with the right number of parameters needed\nwell, it does kind of have to do with `line`\nparameters are the inputs need for a kind of \"transformation\" function. for example `w*x + b`. you have an input value, the transformation function, and the parameters for the transformation. at least that's how I'm thinking about it\nAn *expectant* function is the `(xs, ys) => ...` piece: it's a function that expects a data set as arguments\nAn *objective* function is the function which takes a parameter set and returns a scalar representing the *loss*\nIn the above code we output two losses: the first one with Θ\\_{ 0} set to `0` and the second with Θ\\_{ 0} to `0.0099`. We increase it by a small number to figure out a *rate of change* (which is relative to this arbitrary small amount)\nThe rant of change here is `-0.62 / 0.0099 = -62.63`\nThis helps \"seed\" our rate of learning because otherwise we'd have no idea how fast changing the parameters changes the loss\nGiven the rate of change derived from this arbitrary small value, we use *another* small value and multiple it by this rate of change\nThis is called the **learning rate** and is represented by ⍺. Let's use `0.01`\nIt's basically a \"step size\"\n⍺ \\* -62.63 = -0.6263\nrender:: js-element\n",1],[57,1,[1,"{}"],1],[57,2,55,1],[57,3,1,1],[57,4,"Θ is the parameter set, it has nothing to do with the line function. It's just a list or vector with the right number of parameters needed\nwell, it does kind of have to do with line\nparameters are the inputs need for a kind of \"transformation\" function. for example w*x + b. you have an input value, the transformation function, and the parameters for the transformation. at least that's how I'm thinking about it\nAn expectant function is the (xs, ys) => ... piece: it's a function that expects a data set as arguments\nAn objective function is the function which takes a parameter set and returns a scalar representing the loss\nIn the above code we output two losses: the first one with Θ_{ 0} set to 0 and the second with Θ_{ 0} to 0.0099. We increase it by a small number to figure out a rate of change (which is relative to this arbitrary small amount)\nThe rant of change here is -0.62 / 0.0099 = -62.63\nThis helps \"seed\" our rate of learning because otherwise we'd have no idea how fast changing the parameters changes the loss\nGiven the rate of change derived from this arbitrary small value, we use another small value and multiple it by this rate of change\nThis is called the learning rate and is represented by ⍺. Let's use 0.01\nIt's basically a \"step size\"\n⍺ * -62.63 = -0.6263\nrender:: js-element",1],[57,5,"paragraph",1],[57,6,"d768b44a-c508-4a83-b496-c7af56079e7a",1],[58,0,"```js\nreturn log(output => {\n  output(l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(0.6263, 0))\n})\n```\n",1],[58,1,[1,"{\"lang\" \"js\"}"],1],[58,2,56,1],[58,3,1,1],[58,4,"return log(output => {\n  output(l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(0.6263, 0))\n})",1],[58,5,"code",1],[58,6,"388ad4f5-8389-4dbb-bc72-05fc766d8ac0",1],[59,0,"The rate of change depends on Θ\\_{ 0}. Now that we are using a new value for it, we need to get the new rate of change by using the `0.0099` constant again. Rate of change here is `-25.12`\nIt's not great to derive the rate of change this way though\n",1],[59,1,[1,"{}"],1],[59,2,57,1],[59,3,1,1],[59,4,"The rate of change depends on Θ_{ 0}. Now that we are using a new value for it, we need to get the new rate of change by using the 0.0099 constant again. Rate of change here is -25.12\nIt's not great to derive the rate of change this way though",1],[59,5,"paragraph",1],[59,6,"287086ce-312b-4d0c-9828-e2658029b9c1",1],[60,0,"### Chapter 4\n",1],[60,1,[1,"{}"],1],[60,2,58,1],[60,3,1,1],[60,4,"Chapter 4",1],[60,5,"heading",1],[60,6,"5ccc8de5-e639-4a7f-9067-d11c0edb297f",1],[61,0,"Graph of loss where X is Θ\\_{ 0}:\nrender:: js-element\nsource:: true\n",1],[61,1,[1,"{}"],1],[61,2,59,1],[61,3,1,1],[61,4,"Graph of loss where X is Θ_{ 0}:\nrender:: js-element\nsource:: true",1],[61,5,"paragraph",1],[61,6,"f5e67ffb-9362-4593-8b5e-a55ac1b767dc",1],[62,0,"```js\nreturn graph(x => l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(x, 0),\n            {domainX: [-1, 5]})\n```\n",1],[62,1,[1,"{\"lang\" \"js\"}"],1],[62,2,60,1],[62,3,1,1],[62,4,"return graph(x => l2loss(line)([2, 1, 4, 3], [1.8, 1.2, 4.2, 3.3])(x, 0),\n            {domainX: [-1, 5]})",1],[62,5,"code",1],[62,6,"6aa804da-5c47-482b-8f9d-6bba2e78911b",1],[63,0,"We also refer to the x-axis as **weight**\nRate of change is important. It's called **gradient** and we can define a function to find it for any function\nA gradient function takes a function and a parameter set, and returns a list of gradients for each parameter in the parameter set\nThis is also known as ∇ (del)\n",1],[63,1,[1,"{}"],1],[63,2,61,1],[63,3,1,1],[63,4,"We also refer to the x-axis as weight\nRate of change is important. It's called gradient and we can define a function to find it for any function\nA gradient function takes a function and a parameter set, and returns a list of gradients for each parameter in the parameter set\nThis is also known as ∇ (del)",1],[63,5,"paragraph",1],[63,6,"4a91b287-daee-442b-87f2-2f3a42e6a0f1",1],[64,0,"### My own break: automatic differentiation\n",1],[64,1,[1,"{}"],1],[64,2,62,1],[64,3,1,1],[64,4,"My own break: automatic differentiation",1],[64,5,"heading",1],[64,6,"c1c135c2-0516-4d63-8ac1-09f6722e23cf",1],[65,0,"It turns out the book never defines ∇ which makes it difficult to continue to build running examples. It casually defines it as a function that gets the gradient (or rate of change) at a specific value of a function. I was annoyed that it never gave a definition.\nAfter doing some research, it turns out that ∇ essentially the derivative of a one-dimensional function. Defining this isn't simple, so the book assumes this already provided. So far the book has done a great job building things up from scratch, so this was a confusing turning point and it should have done a better job explaining exactly what ∇ is and why it's not providing a definition. At least point the reader to where they can find more information about it.\nTo define ∇ we need to implement **automatic differentiation**. This is a way to deriving an expression  automatically. We build up a graph of operators and use rules of differentiation to get a new graph of operators representing the differentiated expression.\nI'm using [this observable](https://observablehq.com/@grjzwaan/building-autodiff-from-scratch) as inspiration which in turn is inspired by [micrograd](https://github.com/karpathy/micrograd) from Andrej Karpathy.\nThis uses a neat technique called backwards differentiation which is possible because of a few constraints: we don't need a *full* derivative, but just a derivative of a single variable, and we never need the derivative of a derivative.\nThe basic idea is we want to know for a given expression that uses a variable *x*, how much does changing *x* affect the result? We can figure this out with a few steps:\nConstruct a graph of operations, compute the final value along the way eagerly (the forward pass)\nGiven the final value, perform a backwards pass to compute the amount that each piece of the expression contributed to the final result. Each operation implements its own backwards calculate according to various mathematical rules\nThe value for the single variable `x` in the expression given by the backwards pass is the gradient, or rate of change, for that variable.\nFirst we define a `Value` class that represents the a single value in an equation, and then we define a couple operations like `vadd`, `vmul`, `vpow`, and more to operate on these values. (The `v` prefix means we're operating on values)\nrender:: js-element\n",1],[65,1,[1,"{}"],1],[65,2,63,1],[65,3,1,1],[65,4,"It turns out the book never defines ∇ which makes it difficult to continue to build running examples. It casually defines it as a function that gets the gradient (or rate of change) at a specific value of a function. I was annoyed that it never gave a definition.\nAfter doing some research, it turns out that ∇ essentially the derivative of a one-dimensional function. Defining this isn't simple, so the book assumes this already provided. So far the book has done a great job building things up from scratch, so this was a confusing turning point and it should have done a better job explaining exactly what ∇ is and why it's not providing a definition. At least point the reader to where they can find more information about it.\nTo define ∇ we need to implement automatic differentiation. This is a way to deriving an expression  automatically. We build up a graph of operators and use rules of differentiation to get a new graph of operators representing the differentiated expression.\nI'm using this observable as inspiration which in turn is inspired by micrograd from Andrej Karpathy.\nThis uses a neat technique called backwards differentiation which is possible because of a few constraints: we don't need a full derivative, but just a derivative of a single variable, and we never need the derivative of a derivative.\nThe basic idea is we want to know for a given expression that uses a variable x, how much does changing x affect the result? We can figure this out with a few steps:\nConstruct a graph of operations, compute the final value along the way eagerly (the forward pass)\nGiven the final value, perform a backwards pass to compute the amount that each piece of the expression contributed to the final result. Each operation implements its own backwards calculate according to various mathematical rules\nThe value for the single variable x in the expression given by the backwards pass is the gradient, or rate of change, for that variable.\nFirst we define a Value class that represents the a single value in an equation, and then we define a couple operations like vadd, vmul, vpow, and more to operate on these values. (The v prefix means we're operating on values)\nrender:: js-element",1],[65,5,"paragraph",1],[65,6,"e48b341a-d1b1-46f6-ac8d-494c47a1d318",1],[66,0,"```js\nwindow.Value = class Value {\n  constructor(data, _deps = [], _op = '') {\n    this.data = data;\n    this.grad = 0;\n\n    this._dependencies = _deps;\n    this._op = _op;\n    this._order = [];\n  }\n\n  _backward() {\n    // Reimplemented by the operators.\n  }\n\n  zeroGrad() {\n    this.grad = 0;\n  }\n\n  backward(focus) {\n    // Find the topological ordering of the values\n    // For each value, visit it's dependencies first and add them to the ordering\n    let order = [];\n    let visited = [];\n\n    function get_deps(focus) {\n      if (!visited.includes(focus)) {\n        visited.push(focus);\n        focus._dependencies.forEach(dep => get_deps(dep));\n\n        order.push(focus);\n      }\n    }\n\n    get_deps(this);\n\n    // Initialize the gradient of the whole function to 1\n    // and calculate the gradient for each value.\n    this.grad = 1;\n\n    for (let i = 1; i <= order.length; i++) {\n      let focus = order[order.length - i];\n      focus._backward();\n    }\n\n    this._order = order;\n  }\n}\n\nwindow.vpow = function vpow(base, power) {\n  let val = new Value(base.data ** power, [base], `^${power}`);\n  val._backward = function() {\n    base.grad += power * base.data ** (power - 1) * val.grad;\n  };\n  return val;\n}\n\nwindow.vneg = function vneg(value) {\n  return vmul(value, new Value(-1));\n}\n\nwindow.vadd = function vadd(left, right) {\n  let val = new Value(left.data + right.data, [left, right], '+');\n  val._backward = function() {\n    left.grad += val.grad;\n    right.grad += val.grad;\n  };\n  return val;\n}\n\nwindow.vsub = function vsub(left, right) {\n  return vadd(left, vneg(right));\n}\n\nwindow.vmul = function vmul(left, right) {\n  let val = new Value(left.data*right.data, [left, right], '*');\n  val._backward = function() {\n    left.grad += val.grad * right.data;\n    right.grad += val.grad * left.data;\n  }\n  return val;\n}\n\nconst div = document.createElement('div');\ndiv.innerHTML = '<code>[class Value]</code>'\nreturn div\n```\n",1],[66,1,[1,"{\"lang\" \"js\"}"],1],[66,2,64,1],[66,3,1,1],[66,4,"window.Value = class Value {\n  constructor(data, _deps = [], _op = '') {\n    this.data = data;\n    this.grad = 0;\n\n    this._dependencies = _deps;\n    this._op = _op;\n    this._order = [];\n  }\n\n  _backward() {\n    // Reimplemented by the operators.\n  }\n\n  zeroGrad() {\n    this.grad = 0;\n  }\n\n  backward(focus) {\n    // Find the topological ordering of the values\n    // For each value, visit it's dependencies first and add them to the ordering\n    let order = [];\n    let visited = [];\n\n    function get_deps(focus) {\n      if (!visited.includes(focus)) {\n        visited.push(focus);\n        focus._dependencies.forEach(dep => get_deps(dep));\n\n        order.push(focus);\n      }\n    }\n\n    get_deps(this);\n\n    // Initialize the gradient of the whole function to 1\n    // and calculate the gradient for each value.\n    this.grad = 1;\n\n    for (let i = 1; i <= order.length; i++) {\n      let focus = order[order.length - i];\n      focus._backward();\n    }\n\n    this._order = order;\n  }\n}\n\nwindow.vpow = function vpow(base, power) {\n  let val = new Value(base.data ** power, [base], `^${power}`);\n  val._backward = function() {\n    base.grad += power * base.data ** (power - 1) * val.grad;\n  };\n  return val;\n}\n\nwindow.vneg = function vneg(value) {\n  return vmul(value, new Value(-1));\n}\n\nwindow.vadd = function vadd(left, right) {\n  let val = new Value(left.data + right.data, [left, right], '+');\n  val._backward = function() {\n    left.grad += val.grad;\n    right.grad += val.grad;\n  };\n  return val;\n}\n\nwindow.vsub = function vsub(left, right) {\n  return vadd(left, vneg(right));\n}\n\nwindow.vmul = function vmul(left, right) {\n  let val = new Value(left.data*right.data, [left, right], '*');\n  val._backward = function() {\n    left.grad += val.grad * right.data;\n    right.grad += val.grad * left.data;\n  }\n  return val;\n}\n\nconst div = document.createElement('div');\ndiv.innerHTML = '<code>[class Value]</code>'\nreturn div",1],[66,5,"code",1],[66,6,"fd84e695-e6fb-4570-b30e-4ebf2fd5e91e",1],[67,0,"Using it looks like this. This is how we represent the equation `(x + -4)^2 + 10`:\n",1],[67,1,[1,"{}"],1],[67,2,65,1],[67,3,1,1],[67,4,"Using it looks like this. This is how we represent the equation (x + -4)^2 + 10:",1],[67,5,"paragraph",1],[67,6,"69c831a8-3fab-43a0-998b-f76124bd2e4b",1],[68,0,"```js\nconst variable = new Value(x)\nvadd(vpow(vadd(variable, new Value(-4)), 2), new Value(10))\n```\n",1],[68,1,[1,"{\"lang\" \"js\"}"],1],[68,2,66,1],[68,3,1,1],[68,4,"const variable = new Value(x)\nvadd(vpow(vadd(variable, new Value(-4)), 2), new Value(10))",1],[68,5,"code",1],[68,6,"454aa02f-0992-4d50-ab7a-3b69ef246a02",1],[69,0,"Notice how there isn't really any difference between the `x` value and the other constants in here. They all operate as a `Value`. This simplifies our work and makes it more efficient, but it's important to remember they actually are different. `x` here is an actual variable and needs to be the single input into the equation, and if that's the case reading the `x.grad` value makes sense.\nThe other numbers like `-4` and `10` are *constants* and it wouldn't make any sense to read the `grad` value of them. It took me a while to understand this: if you just did `new Value(4)` to represent `y=4`, which would simply render a horizontal line, and then ran the backwards pass and got the gradient, the value would be 1! The issue is we are assuming the value is *the input variable* and if it were, 1 would be the correct slope because we'd actually be working with the `y=x` equation.\nLet's visualize this. This graph shows the equation `(x - 4)^2 + 10`, and we apply automatic differentiation at each whole number to visualize the gradient at that point. Click \"view source\" to see how this works:\nrender:: js-element\nsource:: true\n",1],[69,1,[1,"{}"],1],[69,2,67,1],[69,3,1,1],[69,4,"Notice how there isn't really any difference between the x value and the other constants in here. They all operate as a Value. This simplifies our work and makes it more efficient, but it's important to remember they actually are different. x here is an actual variable and needs to be the single input into the equation, and if that's the case reading the x.grad value makes sense.\nThe other numbers like -4 and 10 are constants and it wouldn't make any sense to read the grad value of them. It took me a while to understand this: if you just did new Value(4) to represent y=4, which would simply render a horizontal line, and then ran the backwards pass and got the gradient, the value would be 1! The issue is we are assuming the value is the input variable and if it were, 1 would be the correct slope because we'd actually be working with the y=x equation.\nLet's visualize this. This graph shows the equation (x - 4)^2 + 10, and we apply automatic differentiation at each whole number to visualize the gradient at that point. Click \"view source\" to see how this works:\nrender:: js-element\nsource:: true",1],[69,5,"paragraph",1],[69,6,"e9c241bb-9fe6-48c0-bbcf-90dbee20cd8d",1],[70,0,"```js\nconst gradients = range(-4, 12).map(x => {\n  const target = new Value(x);\n  const top = vadd(vpow(vadd(target, new Value(-4)), 2), new Value(10));\n  top.backward();\n\n  return v => (v - target.data) * target.grad + top.data\n})\n\nreturn graph(\n  ...gradients,\n  x => Math.pow(x - 4, 2) + 10,\n  {domainX: [-5, 15],\n   colors: [...gradients.map(_ => \"#a0a0a0\"), \"#e60049\"]}\n)\n```\n",1],[70,1,[1,"{\"lang\" \"js\"}"],1],[70,2,68,1],[70,3,1,1],[70,4,"const gradients = range(-4, 12).map(x => {\n  const target = new Value(x);\n  const top = vadd(vpow(vadd(target, new Value(-4)), 2), new Value(10));\n  top.backward();\n\n  return v => (v - target.data) * target.grad + top.data\n})\n\nreturn graph(\n  ...gradients,\n  x => Math.pow(x - 4, 2) + 10,\n  {domainX: [-5, 15],\n   colors: [...gradients.map(_ => \"#a0a0a0\"), \"#e60049\"]}\n)",1],[70,5,"code",1],[70,6,"37463e4b-cf82-41df-9ddb-133a7b14aebe",1],[71,0,"Feels really good to understand this and be able to work with a real system written from scratch. I'll be able to continue to run examples from the book!\nHowever, first we need to extend our tensor functions to support our now automatic differentiation system. Remember when we defined `tmul`, `tadd`, etc? That feels very similar to our new operation above right? We can combine both together into new tensor operations that support both properties: extended math functions that work with tensors that also support automatic differentiation.\nrender:: js-element\n",1],[71,1,[1,"{}"],1],[71,2,69,1],[71,3,1,1],[71,4,"Feels really good to understand this and be able to work with a real system written from scratch. I'll be able to continue to run examples from the book!\nHowever, first we need to extend our tensor functions to support our now automatic differentiation system. Remember when we defined tmul, tadd, etc? That feels very similar to our new operation above right? We can combine both together into new tensor operations that support both properties: extended math functions that work with tensors that also support automatic differentiation.\nrender:: js-element",1],[71,5,"paragraph",1],[71,6,"a4c0a02c-badb-4974-96f0-ebc9dd6796b4",1],[72,0,"```js\nwindow.scalarp = v => typeof v === 'number' || v instanceof Value\n\nwindow._binop = (op, t1, t2) => {\n  const r1 = rank(t1);\n  const r2 = rank(t2);\n  \n  if(r1 === r2) {\n    if(scalarp(t1)) {\n      return op(t1, t2);\n    }\n    return t1.map((v, idx) => _binop(op, v, t2[idx]))\n  }\n  else if(r1 < r2) {\n    return t2.map(t => _binop(op, t1, t))\n  }\n  else {\n    return t1.map(t => _binop(op, t, t2))\n  }\n}\n\nwindow._unaryop = (op, t1) => {\n  if(scalarp(t1)) {\n    return op(t1);\n  }\n  return t1.map(v => _unaryop(op, v))\n}\n\nwindow.tsub_ = (t1, t2) => _binop(vsub, t1, t2);\nwindow.tadd_ = (t1, t2) => _binop(vadd, t1, t2);\nwindow.tmul_ = (t1, t2) => _binop(vmul, t1, t2);\nwindow.tsqr_ = t => _unaryop(v => vpow(v, 2), t);\n\nwindow.tsum_ = t => {\n  if(rank(t) > 1) {\n    return t.map(x => tsum_(x))\n  }\n  \n  let total = new Value(0);\n  for(let i=0; i<t.length; i++) {\n    total = vadd(total, t[i])\n  }\n  return total\n}\n\nwindow.tdual_ = t => _unaryop(v => new Value(v), t)\nwindow.tdata1 = t => t.map(v => v.data)\nwindow.tgrad1 = t => t.map(v => v.grad)\n\nwindow.line = x => (w, b) => tadd_(tmul_(w, x), b)\n\nwindow.l2loss = target => (\n  (xs, ys) => {\n    return (...params) => {\n      const pred_ys = target(xs)(params[0], params[1]);\n      return tsum_(tsqr_(tsub_(ys, pred_ys)));\n    }\n  }\n);\n\nwindow.gradient_of = (obj, params) => {\n  const duals = params.map(p => new Value(p))\n  const v = obj(...duals);\n  v.backward();\n  return duals.map(d => d.grad)\n}\n\nreturn log(output => {\n  const gs = gradient_of(\n    l2loss(line)(tdual_([2, 1, 4, 3]), tdual_([1.8, 1.2, 4.2, 3.3])),\n    [0, 0]\n  )\n  \n  output(gs, [-63, -21])\n})\n```\n",1],[72,1,[1,"{\"lang\" \"js\"}"],1],[72,2,70,1],[72,3,1,1],[72,4,"window.scalarp = v => typeof v === 'number' || v instanceof Value\n\nwindow._binop = (op, t1, t2) => {\n  const r1 = rank(t1);\n  const r2 = rank(t2);\n  \n  if(r1 === r2) {\n    if(scalarp(t1)) {\n      return op(t1, t2);\n    }\n    return t1.map((v, idx) => _binop(op, v, t2[idx]))\n  }\n  else if(r1 < r2) {\n    return t2.map(t => _binop(op, t1, t))\n  }\n  else {\n    return t1.map(t => _binop(op, t, t2))\n  }\n}\n\nwindow._unaryop = (op, t1) => {\n  if(scalarp(t1)) {\n    return op(t1);\n  }\n  return t1.map(v => _unaryop(op, v))\n}\n\nwindow.tsub_ = (t1, t2) => _binop(vsub, t1, t2);\nwindow.tadd_ = (t1, t2) => _binop(vadd, t1, t2);\nwindow.tmul_ = (t1, t2) => _binop(vmul, t1, t2);\nwindow.tsqr_ = t => _unaryop(v => vpow(v, 2), t);\n\nwindow.tsum_ = t => {\n  if(rank(t) > 1) {\n    return t.map(x => tsum_(x))\n  }\n  \n  let total = new Value(0);\n  for(let i=0; i<t.length; i++) {\n    total = vadd(total, t[i])\n  }\n  return total\n}\n\nwindow.tdual_ = t => _unaryop(v => new Value(v), t)\nwindow.tdata1 = t => t.map(v => v.data)\nwindow.tgrad1 = t => t.map(v => v.grad)\n\nwindow.line = x => (w, b) => tadd_(tmul_(w, x), b)\n\nwindow.l2loss = target => (\n  (xs, ys) => {\n    return (...params) => {\n      const pred_ys = target(xs)(params[0], params[1]);\n      return tsum_(tsqr_(tsub_(ys, pred_ys)));\n    }\n  }\n);\n\nwindow.gradient_of = (obj, params) => {\n  const duals = params.map(p => new Value(p))\n  const v = obj(...duals);\n  v.backward();\n  return duals.map(d => d.grad)\n}\n\nreturn log(output => {\n  const gs = gradient_of(\n    l2loss(line)(tdual_([2, 1, 4, 3]), tdual_([1.8, 1.2, 4.2, 3.3])),\n    [0, 0]\n  )\n  \n  output(gs, [-63, -21])\n})",1],[72,5,"code",1],[72,6,"0429b467-1a8c-45dc-b43d-4bfc45adf598",1],[73,0,"That works!\n",1],[73,1,[1,"{}"],1],[73,2,71,1],[73,3,1,1],[73,4,"That works!",1],[73,5,"paragraph",1],[73,6,"5e3c79c3-b584-4d95-81d2-806a8e9234a9",1],[74,0,"### Chapter 4 (continued)\n",1],[74,1,[1,"{}"],1],[74,2,72,1],[74,3,1,1],[74,4,"Chapter 4 (continued)",1],[74,5,"heading",1],[74,6,"f54032f4-8102-4ffa-9917-a219c207cbfc",1],[75,0,"Revisions\n**4.23**: define a `revise` function which helps revise parameters given a function\nrender:: js-element\nsource:: true\n",1],[75,1,[1,"{}"],1],[75,2,73,1],[75,3,1,1],[75,4,"Revisions\n4.23: define a revise function which helps revise parameters given a function\nrender:: js-element\nsource:: true",1],[75,5,"paragraph",1],[75,6,"924598b1-beb0-436f-9e31-0ad8cfd75caf",1],[76,0,"```js\nwindow.revise = function revise(func, revs, params) {\n  while(revs > 0) {\n    params = func(params)\n    revs--;\n  }\n  return params;\n}\n\nreturn log(output => {\n  output(\n    revise(\n      params => params.map(p => p - 3),\n      5,\n      [1, 2, 3]\n    ),\n    [-14, -13, -12]\n  )\n})\n```\n",1],[76,1,[1,"{\"lang\" \"js\"}"],1],[76,2,74,1],[76,3,1,1],[76,4,"window.revise = function revise(func, revs, params) {\n  while(revs > 0) {\n    params = func(params)\n    revs--;\n  }\n  return params;\n}\n\nreturn log(output => {\n  output(\n    revise(\n      params => params.map(p => p - 3),\n      5,\n      [1, 2, 3]\n    ),\n    [-14, -13, -12]\n  )\n})",1],[76,5,"code",1],[76,6,"0a138f6a-b30b-42b1-bfc1-5e5294edef41",1],[77,0,"^9ae15b\n",1],[77,1,[1,"{}"],1],[77,2,75,1],[77,3,1,1],[77,4,"^9ae15b",1],[77,5,"paragraph",1],[77,6,"c44086ab-6ff2-4e3d-8afa-b0a1c982e81b",1],[78,0,"Now let's use this `revise` function to help adjust parameters to reduce loss\nrender:: js-element\nsource:: true\n",1],[78,1,[1,"{}"],1],[78,2,76,1],[78,3,1,1],[78,4,"Now let's use this revise function to help adjust parameters to reduce loss\nrender:: js-element\nsource:: true",1],[78,5,"paragraph",1],[78,6,"6f78ebe6-de19-4215-976a-b821454cc669",1],[79,0,"```js\nconst learning_rate = 0.01\nconst obj = l2loss(line)(tdual_([2, 1, 4, 3]), tdual_([1.8, 1.2, 4.2, 3.3]))\n\nwindow.lineParams = revise(\n  params => {\n    const gs = gradient_of(obj, params)\n    return [params[0] - gs[0] * learning_rate,\n            params[1] - gs[1] * learning_rate]\n  },\n  1000,\n  [0, 0]\n)\n\nreturn log(output => output(lineParams))\n```\n",1],[79,1,[1,"{\"lang\" \"js\"}"],1],[79,2,77,1],[79,3,1,1],[79,4,"const learning_rate = 0.01\nconst obj = l2loss(line)(tdual_([2, 1, 4, 3]), tdual_([1.8, 1.2, 4.2, 3.3]))\n\nwindow.lineParams = revise(\n  params => {\n    const gs = gradient_of(obj, params)\n    return [params[0] - gs[0] * learning_rate,\n            params[1] - gs[1] * learning_rate]\n  },\n  1000,\n  [0, 0]\n)\n\nreturn log(output => output(lineParams))",1],[79,5,"code",1],[79,6,"bafc502b-e867-4c93-b0cd-1ad8e0c41dff",1],[80,0,"We found our line! Let's plot it against the xs, ys dataset and see how it fits:\nrender:: js-element\n",1],[80,1,[1,"{}"],1],[80,2,78,1],[80,3,1,1],[80,4,"We found our line! Let's plot it against the xs, ys dataset and see how it fits:\nrender:: js-element",1],[80,5,"paragraph",1],[80,6,"5f536195-3d3e-40e3-a922-05357fe20c69",1],[81,0,"```js\nconst xs = [2, 1, 4, 3]\nconst ys = [1.8, 1.2, 4.2, 3.3]\nconst params = window.lineParams;\n\nreturn graph(x => params[0] * x + params[1], {\n  marks: Plot.dot(zip(xs, ys), {x: n => n[0], y: n => n[1], fill: 'black'}),\n  domainX: [0, 5],\n  domainY: [0, 5]\n})\n```\n",1],[81,1,[1,"{\"lang\" \"js\"}"],1],[81,2,79,1],[81,3,1,1],[81,4,"const xs = [2, 1, 4, 3]\nconst ys = [1.8, 1.2, 4.2, 3.3]\nconst params = window.lineParams;\n\nreturn graph(x => params[0] * x + params[1], {\n  marks: Plot.dot(zip(xs, ys), {x: n => n[0], y: n => n[1], fill: 'black'}),\n  domainX: [0, 5],\n  domainY: [0, 5]\n})",1],[81,5,"code",1],[81,6,"1766248c-4c84-425f-9606-6224ad8c9a9c",1],[82,0,"Hell yeah!\nThis is the optimization of **gradient descent**. We used a learning rate of `0.01` with our loss function and revised 1000 times to find the parameter set that fits the dataset bet\nSo far we've hardcoded the amount of parameters in our `revise` usage, let's generalize that. We can also generalize the other constants like learning rate and objective function into a general `gradient_descent` function:\nrender:: js\nsource:: true\n",1],[82,1,[1,"{}"],1],[82,2,80,1],[82,3,1,1],[82,4,"Hell yeah!\nThis is the optimization of gradient descent. We used a learning rate of 0.01 with our loss function and revised 1000 times to find the parameter set that fits the dataset bet\nSo far we've hardcoded the amount of parameters in our revise usage, let's generalize that. We can also generalize the other constants like learning rate and objective function into a general gradient_descent function:\nrender:: js\nsource:: true",1],[82,5,"paragraph",1],[82,6,"a2a06b83-40f9-44b7-b780-0edb2e411c86",1],[83,0,"```js\nwindow.gradient_descent = function gradient_descent(obj, initialp, α, revs) {\n  return revise(\n    params => {\n      const gs = gradient_of(obj, params)\n      return gs.map((g, idx) => params[idx] - g * α)\n    },\n    revs,\n    initialp\n  )\n}\n```\n",1],[83,1,[1,"{\"lang\" \"js\"}"],1],[83,2,81,1],[83,3,1,1],[83,4,"window.gradient_descent = function gradient_descent(obj, initialp, α, revs) {\n  return revise(\n    params => {\n      const gs = gradient_of(obj, params)\n      return gs.map((g, idx) => params[idx] - g * α)\n    },\n    revs,\n    initialp\n  )\n}",1],[83,5,"code",1],[83,6,"1798d80a-cdb6-473c-a0ed-d1f9c4eba89d",1],[84,0,"```js render=\"js-element\" source\nreturn log(output => {\n  const obj = l2loss(line)(tdual_([2, 1, 4, 3]), tdual_([1.8, 1.2, 4.2, 3.3]))\n  output(gradient_descent(obj, [0, 0], 0.01, 1000))  \n})\n```\n",1],[84,1,[1,"{\"lang\" \"js\", \"render\" \"js-element\", \"source\" true}"],1],[84,2,82,1],[84,3,1,1],[84,4,"return log(output => {\n  const obj = l2loss(line)(tdual_([2, 1, 4, 3]), tdual_([1.8, 1.2, 4.2, 3.3]))\n  output(gradient_descent(obj, [0, 0], 0.01, 1000))  \n})",1],[84,5,"code",1],[84,6,"ffc2fbb0-0e67-4435-813c-dc153e926981",1],[85,0,"Θ is the symbol used for the revised parameters passed to the revision function\nNotice the difference between Θ and θ\n",1],[85,1,[1,"{}"],1],[85,2,83,1],[85,3,1,1],[85,4,"Θ is the symbol used for the revised parameters passed to the revision function\nNotice the difference between Θ and θ",1],[85,5,"paragraph",1],[85,6,"7e5fe664-0f85-4754-96f5-daaf7a1701f1",1],[86,0,"### Interlude II\n",1],[86,1,[1,"{}"],1],[86,2,84,1],[86,3,1,1],[86,4,"Interlude II",1],[86,5,"heading",1],[86,6,"f1df2cf7-0460-42e9-8336-dde2296d01d6",1],[87,0,"\\[\\[Welcome here -- the one thing that could#^9ae15b]]\n",1],[87,1,[1,"{}"],1],[87,2,85,1],[87,3,1,1],[87,4,"[[Welcome here -- the one thing that could#^9ae15b]]",1],[87,5,"paragraph",1],[87,6,"861498ba-0c99-4e09-b482-928bc216698a",1],[88,0,"```comment\nwhat I could do\n```\n",1],[88,1,[1,"{\"lang\" \"comment\"}"],1],[88,2,86,1],[88,3,1,1],[88,4,"what I could do",1],[88,5,"code",1],[88,6,"6775737d-4dfb-4d3c-b6ed-b6c4c47d59f1",1],[89,0,"```html render=true source=true depends=\"<id>\"\n<div> foo </div>\n```\n",1],[89,1,[1,"{\"lang\" \"html\", \"render\" true, \"source\" true, \"depends\" \"<id>\"}"],1],[89,2,87,1],[89,3,1,1],[89,4,"<div> foo </div>",1],[89,5,"code",1],[89,6,"d06387cb-1a95-432c-b2a2-f5b6f15573b9",1]],"aevt":[6,12,19,26,33,40,47,54,61,68,75,82,89,96,103,110,117,124,131,138,145,152,159,166,173,180,187,194,201,208,215,222,229,236,243,250,257,264,271,278,285,292,299,306,313,320,327,334,341,348,355,362,369,376,383,390,397,404,411,418,425,432,439,446,453,460,467,474,481,488,495,502,509,516,523,530,537,544,551,558,565,572,579,586,593,600,607,614,7,13,20,27,34,41,48,55,62,69,76,83,90,97,104,111,118,125,132,139,146,153,160,167,174,181,188,195,202,209,216,223,230,237,244,251,258,265,272,279,286,293,300,307,314,321,328,335,342,349,356,363,370,377,384,391,398,405,412,419,426,433,440,447,454,461,468,475,482,489,496,503,510,517,524,531,538,545,552,559,566,573,580,587,594,601,608,615,14,21,28,35,42,49,56,63,70,77,84,91,98,105,112,119,126,133,140,147,154,161,168,175,182,189,196,203,210,217,224,231,238,245,252,259,266,273,280,287,294,301,308,315,322,329,336,343,350,357,364,371,378,385,392,399,406,413,420,427,434,441,448,455,462,469,476,483,490,497,504,511,518,525,532,539,546,553,560,567,574,581,588,595,602,609,616,8,15,22,29,36,43,50,57,64,71,78,85,92,99,106,113,120,127,134,141,148,155,162,169,176,183,190,197,204,211,218,225,232,239,246,253,260,267,274,281,288,295,302,309,316,323,330,337,344,351,358,365,372,379,386,393,400,407,414,421,428,435,442,449,456,463,470,477,484,491,498,505,512,519,526,533,540,547,554,561,568,575,582,589,596,603,610,617,9,16,23,30,37,44,51,58,65,72,79,86,93,100,107,114,121,128,135,142,149,156,163,170,177,184,191,198,205,212,219,226,233,240,247,254,261,268,275,282,289,296,303,310,317,324,331,338,345,352,359,366,373,380,387,394,401,408,415,422,429,436,443,450,457,464,471,478,485,492,499,506,513,520,527,534,541,548,555,562,569,576,583,590,597,604,611,618,10,17,24,31,38,45,52,59,66,73,80,87,94,101,108,115,122,129,136,143,150,157,164,171,178,185,192,199,206,213,220,227,234,241,248,255,262,269,276,283,290,297,304,311,318,325,332,339,346,353,360,367,374,381,388,395,402,409,416,423,430,437,444,451,458,465,472,479,486,493,500,507,514,521,528,535,542,549,556,563,570,577,584,591,598,605,612,619,11,18,25,32,39,46,53,60,67,74,81,88,95,102,109,116,123,130,137,144,151,158,165,172,179,186,193,200,207,214,221,228,235,242,249,256,263,270,277,284,291,298,305,312,319,326,333,340,347,354,361,368,375,382,389,396,403,410,417,424,431,438,445,452,459,466,473,480,487,494,501,508,515,522,529,536,543,550,557,564,571,578,585,592,599,606,613,620,0,1,2,3,4,5],"avet":[8,15,22,29,36,43,50,57,64,71,78,85,92,99,106,113,120,127,134,141,148,155,162,169,176,183,190,197,204,211,218,225,232,239,246,253,260,267,274,281,288,295,302,309,316,323,330,337,344,351,358,365,372,379,386,393,400,407,414,421,428,435,442,449,456,463,470,477,484,491,498,505,512,519,526,533,540,547,554,561,568,575,582,589,596,603,610,617]}